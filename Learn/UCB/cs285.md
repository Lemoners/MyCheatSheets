# Lecture 01

# Lecture 04

## 4.1. Definition of MDP 

- Markov chain: $\mathcal{M}=<\mathcal S, \mathcal T>$. (state space and transition operator).
  - Let $\mu_{t,i}=p(s_t=i)$, $\overrightarrow \mu_{t+1}=\mathcal T \overrightarrow \mu_t$
- Markov decision process: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal T, r>$.
- Partially observed MDP: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal O, \mathcal T, \varepsilon, r>$, where $\varepsilon$ stands for emission probability $p(o_t|s_t)$.

## 4.2. Definition of RL problem

- $\underbrace{p_\theta(s_1,a_1,\cdots,s_T,a_T)}_{p_\theta(\tau)}=p(s_1)\mathop\Pi\limits_{t=1}^T\underbrace{\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)}_{\text{Markov chain on (s,a)}}$.

  - Once $\pi_\theta$ is fixed, you can actually view the MDP as a <font color="red">*Markov chain*</font> on an augmented state space (original $\mathcal S$ and $\mathcal A$), $p((s_{t+1},a_{t+1})|(s_t,a_t))=p(s_{t+1}|s_t,a_t)\pi_\theta(a_{t+1}|s_{t+1})$.

- $\theta^*=\arg\max\limits_{\theta}E_{\tau\sim p_{\theta}(\tau)}\left[\sum\limits_{t=1}r(s_t,a_t)\right] $. (Goal)

  - $= \arg\max\limits_{\theta}\sum\limits_{t=1}^TE_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]$. (Markov Chain)
  - And thus $\begin{pmatrix}s_{t+k}\\a_{t+k}\end{pmatrix}=\mathcal T^k\begin{pmatrix}s_{t}\\a_{t}\end{pmatrix}$, if $T=\infty$, will $p(s_t,a_t)$ converge to a <font color="red">*stationary*</font> distribution? (just solve $\mu=\mathcal T \mu$)?
    - Yes, under ergodicity.
    - And the expected return under infinite horizon is **dominated** by the stationary state, and thus the objective is $\arg\max\limits_{\theta}E_{(s,a)\sim p_\theta(s,a)}[r(s,a)]$.

- > Even if you take expectations of non-smooth, non-differentiable nasty horrible functions, they can actually be reasonably behaved in the parameters of the distribution under with you take expectation. $\leftarrow$ why GD can work in RL.

## 4.3. Anatomy of a RL algorithm

- <img src="./pic/image-20201105104915804.png" alt="image-20201105104915804" style="zoom:20%;" />
- Value-based algorithms don't have a essential policy ($\arg\max$ if any).
- How do we deal with *expectations*?
  -  ![image-20201105110020150](./pic/image-20201105110020150.png)
  - <font color="red">But actually it's not that easy to modify $\pi_\theta$, because $Q$ is also dependent on $\pi_\theta$, but this is the intuition of $Q$.</font>
- The value function and the Q-function:
  - <img src="./pic/image-20201105110349477.png" alt="image-20201105110349477" style="zoom: 50%;" />
  - Idea 1: if we know $Q_{\pi_\theta}$, we can improve $\pi$ greedily.
  - Idea 2: if $Q_{\pi_\theta}(s,a)>V_{\pi_\theta}(s)$, then $a$ is **better than average**. 

## 4.4. Brief overview of RL algorithm types

- ![image-20201105110941580](./pic/image-20201105110941580.png)
  - AC is the combination of PG and value-based (it use value function in PG to improve policy).
- **Consideration** for algorithm selection:
  - Sample efficiency.
  - Stability (convergence) and ease use.
  - Assumptions (observability, episodic learning, continuity or smoothness)
- ![image-20201105111439799](./pic/image-20201105111439799.png)

# Lecture 05

- Evaluate the objective: ![image-20201107093448703](./pic/image-20201107093448703.png)

## 5.1. Policy Gradient

- ![image-20210223103332006](.\pic\image-20201107093649374.png)
  - <img src=".\pic\image-20210223103536703.png" alt="image-20210223103536703" style="zoom: 50%;" />
  - **Why** is the reward independent of $\theta$?
    - If the policy change, the expectation of reward will change, which is accounted for at the front term, but once you sampled $s_t,a_t$ from your policy, the reward will only depend on $s_t,a_t$.
- Basic components of RL algorithm (PG) / *REINFORCE* algorithm:
  - <font color="orange">generate samples: just run the policy.</font>
  - <font color="gren">fit a model to estimate return: just run the policy.</font>
  - <font color="blue">improve the policy: $\theta=\theta+\alpha\nabla_\theta J(\theta)$</font>
- PG vs. MLE:
  - $\nabla_\theta log\,\pi_\theta(\tau)$ actually points at the direction that increase $\pi_\theta(\tau)$ (the probability of $\tau$).
  - ![image-20201107095427620](./pic/image-20201107095427620.png)
    - PG only increase the probability of **good** trajectories, while MLE increase the probability of all trajectories. So PG is like *a weighted version* of MLE.
    - <font color="grey">BTW, we might use the built-in MLE-gradient to compute the gradient of PG.</font>
- Markov Properties are **NOT** actually used in PG, so we can deduct the “exact” derivatives at **Partial Observation**:
  - <img src=".\pic\image-20210223104403663.png" alt="image-20210223104403663" style="zoom: 50%;" />
- <font color="red">Why PG won't work?: *the gradient* has high variance!</font>
  - Assume we are trying to tuning a Gaussian policy, and receive three samples, one negative one the left and two positive one the right. $\rightarrow$: penalty(left); encourage(right)
  - ![image-20201107100459679](./pic/image-20201107100459679.png)
  - **After adding a big constant to all samples**  $\rightarrow$: encourage(left); encourage(right), so the policy won't shift that much to the right compared to the above one.
  - ![image-20201107100539826](./pic/image-20201107100539826.png)
  - **What if the two "good" rewards are zero**?
    - You policy (center of the Gaussian) will shift either left or right depending on the previous position of your policy. (either coming from the left or the right).

## 5.2. Basic variance reduction: causality

- *Causality*: The action you choose **now** will **not** affect the rewards you received in the **past**. $\neq$ Markov properties.
  - <img src="./pic/image-20201107101106405.png" alt="image-20201107101106405" style="zoom:30%;" />
- This is a still **unbiased estimation** of PG, but with **more** variance. (see paper for proof)
  - $\sum\limits_{i=1}^N\sum\limits_{t=1}^{T}\nabla_{\theta}log\pi_\theta(a_{i,t}|s_{i,t})(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'})) = 0$
    - For that $E_{\tau\sim p(\theta)}[\nabla_\theta log\, \pi_\theta(\tau)\cdot b] = 0$, as long as $b$ is constant w.r.t. $\theta$ and $\tau$
  - $原式 = \sum\limits_{t=1}^T\mathbb{E}_{s_t,a_t}[\nabla_\theta log\pi_\theta(a_t|s_t)(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'}))]$, and $(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'}))$ is constant w.r.t. $\theta$ and $a_t$.

## 5.3. Basic variance reduction: baselines

- We only want the trajectories which are "better" than average to be more often.
  - ![image-20201107101933959](./pic/image-20201107101933959.png)
  - Still **unbiased**, but will alter the variance.
-  Analyzing variance for acquiring optimal *b*:
  - ![image-20201107104958617](./pic/image-20201107104958617.png)
  - The second term of the variance is independent of $b$, so we only need to focus on the former one.
  - 结论: 最优的baseline是不同$r(\tau)$基于$g(\tau)^2$ (gradient magnitude)的**加权平均**.
    - 注: 不要直接考虑$\nabla_\theta\,log\pi_\theta(\tau)$是向量的情况, 因为那样的话我们就不能仅仅优化Var了,因为Var是矩阵了,优化一个矩阵是一个不明确的操作.
    - 如果$\theta$是向量, 那你可以对它的每一个维度进行上述计算, 所以最后对于$\theta$不同的维度会有不同的optimal $b$.

## 5.4. On-policy via Off-policy

- Importance Sampling (<font color="red">**IS**</font>):
  - <img src="./pic/image-20201107105515950.png" alt="image-20201107105515950" style="zoom:33%;" />
  - **需要满足条件**: $p(x)f(x)$ is always 0 whenever $q(x)=0$.
  
- Problem: the important coefficient will increase the variance:
  - Consider *causality*: (NOTE: $\pi_\theta(\tau)=p(s_1) \mathop\Pi\limits_{t=1}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$)
    - <img src="./pic/image-20201107110753092.png" alt="image-20201107110753092" style="zoom:140%;" />
    - Causality is **NOT** an *arbitrary* modification, it's equal to its previous formulation.
  - Ignoring the latter product will result in the **policy iteration** algorithm.
    - Note that you will get a *different gradient* after this modification, but the new gradient **will still improve** your policy, but for a different reason.
    - ![image-20201107110941927](./pic/image-20201107110941927.png)
  
- A first-order approximation of IS.
  
  - ![image-20201107111350323](./pic/image-20201107111350323.png)
  - The importance coefficient is exponential in $T$, and we can assume it's smaller/bigger than $1$, so the product will decrease to zero / increase to infinity essentially fast ($\Leftrightarrow$ the variance will increase to infinity), which is troublesome.
  - **Differently**: so instead of importance sampling over **entire trajectories**, you can just consider the importance sample over the **state-action joints**. 
  - **Ignore the state portion**: when ignoring the state marginal distribution, you actually didn't get the right gradient, but this will only result in bounded error when $\theta'$ is not too far from $\theta$.

## 5.5. Implementing Policy Gradients

- <img src=".\pic\image-20210223113353534.png" alt="image-20210223113353534" style="zoom: 50%;" />
  - Assume $\theta\in\mathbb{R}^n$ ($n$ can be very high for NN), then direct calculation will require $N\times T\times n$ times.
  - We want to utilize the back propagation. $\rightarrow$ we need a **graph** such that its gradient is the policy gradient.
- ???: YouTube上的代码似乎是一种自己根据REINFORCE定义的东西: $\nabla\mathcal{L}(a_\theta, a_{\theta'})q(a_\theta, a)$

## 5.6. Advanced Policy Gradient

- Numerical issue with PG:
  - Different **gradient magnitude** at different parameters. So the gradient might be dominant by some parameter. e.g. $\nabla_\theta J(\theta)=\begin{bmatrix}1000000\\1\end{bmatrix}$
  - Some parameter **affect the policy** a lot more than others. e.g. $\pi_\theta(s)=\theta_1^{10}s_1+\theta_2s_2$, it's hard to pick one *step size* for all parameters.
- **Covariant/natural Policy Gradient**:
  - The constrained optimization view of PG:
    - <img src=".\pic\image-20210223151427891.png" alt="image-20210223151427891" style="zoom: 50%;" /> $\rightarrow$ this's actually maximize the first order Taylor expansion of $J(\theta')$ at $\theta$.
  - We want the step to be the same in **policy space** rather than in **parameter space**.
    - <img src=".\pic\image-20210223151605579.png" alt="image-20210223151605579" style="zoom:50%;" />
    - We usually use KL-divergence, but to simplify, we use the second order Taylor expansion as surrogate of KL-divergence.
      - <img src=".\pic\image-20210223151816996.png" alt="image-20210223151816996" style="zoom:50%;" />
      - The Fisher-information matrix can be estimated through samples.
    - Finally: <img src=".\pic\image-20210223151925042.png" alt="image-20210223151925042" style="zoom:50%;" />
      - And the solution is <img src="C:\Users\98748\AppData\Roaming\Typora\typora-user-images\image-20210223151953989.png" alt="image-20210223151953989" style="zoom: 50%;" />, where $\alpha$ is your Lagrange multiplier.
  - Related works:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210223152057335.png" alt="image-20210223152057335" style="zoom:35%;" />

# Lecture 06 Actor-critic  Algorithms

<img src="./pic/image-20201121125544218.png" alt="image-20201121125544218" style="zoom:35%;" />

- The key idea for this lecture is to find *better* "reward to go".

## 6.1. Improving the policy gradient with a critic

- Due to the static of RL, a better **"reward to go"** can be an average over possible actions: (and this also bring less variance than single trajectory)
  - ![image-20201121130053471](./pic/image-20201121130053471.png)

- Add a **baseline** (Average of what?$\frac{1}{N}\sum\limits_{i}Q(s_{i,t},a_{i,t}); V(s_t)=E_{a_t\sim\pi_\theta(a_t|s_t)}[Q(s_t,a_t)]$)
  - $A(s_{i,t},a_{i,t}) = Q(s_{i,t},a_{i,t})-V(s_{i,t})$: how better this action is than the average action.
- ![image-20201121130602834](./pic/image-20201121130602834.png)

  - ![image-20201121130707261](./pic/image-20201121130707261.png) 

## 6.2. The policy evaluation problem

- Monte Carlo policy evaluation:
  - $V^{\pi}(s_t)\approx\sum\limits_{t'=t}^Tr(s_{t'},a_{t'})$. (It's a little hard to calculate $\sum\limits_i\sum\limits_{t'=t}^Tr(s_{i,t'},a_{i,t'})$ because this requires to reset the simulator to the exact state $s_{t'}$)
  - Different targets in supervised learning: 
    - ![image-20201121150132050](./pic/image-20201121150132050.png)
    - The ideal target is a slightly incorrect (at initial state it's better to use MC target) but it has a lower variance (especially useful when the environment is very stochastic). 
- <img src="./pic/image-20201121150657423.png" alt="image-20201121150657423" style="zoom:25%;" />

## 6.3. Discount factors

- When $T\rightarrow\infty$, $\bar{V}^{\pi}_{\phi}$ can get infinitely large in many cases. Simple trick. use $\gamma\in[0,1], e.g.\,0.99$, better to get rewards sooner than later.
  - $\gamma$ **changes** the MDP, the agent now has the probability $1-\gamma$ to die at every state.
    - <img src="./pic/image-20201121151040626.png" alt="image-20201121151040626" style="zoom:33%;" />
- PG with discount factors:
  - <img src="./pic/image-20201121151441863.png" alt="image-20201121151441863" style="zoom: 40%;" />
  - The critic is actually **equals to** option 1:
    - $\sum\limits_{t=t_0}^T\gamma^{t-t_0}r(s_t,a_t)=r(s_t,a_t)+\gamma\hat{V}_\phi^\pi(s_{t+1})$
  - And if we change the form of option 2:
    - ![image-20201121151740629](./pic/image-20201121151740629.png)
    - The latter gradients are also smaller due to the discount factors.
  - Option 1 is we frequently use, while option 2 is actually dealing  with the MDP where you have probability $1-\gamma$ to die at every state.
    - In option 1, $\gamma$ can be viewed as a way to eliminate variance in your value estimator.

## 6.4. The actor-critic algorithm

- Architecture design:
  - ![image-20201121152709737](./pic/image-20201121152709737.png)
    - In shared network design, you have to carefully balance two different gradient (and their magnitudes) so that one won't be blown off by the other.
- **Algorithms**:
  - 
- ![image-20201121153314978](./pic/image-20201121153314978.png)
  - The baseline can depend on *state*, while it's expectation still works out to zero. (因为在做期望时, 外部是对$s_t$,$a_t$的二重期望, 我们可以先对$a_t$做期望, 这样关于baseline的期望就已经是0了)
  - Tricks : Gu et al. 2016 (Q-prop)
- Eligibility traces & n-step returns:
  - ![image-20201121153824244](./pic/image-20201121153824244.png)
  - We want to use $\hat{A}^\pi_{MC}$ in the short term and $\hat{A}^\pi_{C}$ in the long term, one way is to **cut off the tails** in $\hat{A}^\pi_{MC}$ so we won't be affected by the high variance caused by the infinite future.
    - <img src="./pic/image-20201121154042679.png" alt="image-20201121154042679" style="zoom: 25%;" />
    - We can add $\gamma^n\bar{V}_\phi^\pi(s_{t+n})$ to compensate for the cut tail.
      - ![image-20201121154229642](./pic/image-20201121154229642.png)
- GAE:
  - ![image-20201121154732765](./pic/image-20201121154732765.png)

# Lecture 07 Value Function Methods

- 

