# Lecture 01

# Lecture 04

## 4.1. Definition of MDP 

- Markov chain: $\mathcal{M}=<\mathcal S, \mathcal T>$. (state space and transition operator).
  - Let $\mu_{t,i}=p(s_t=i)$, $\overrightarrow \mu_{t+1}=\mathcal T \overrightarrow \mu_t$
- Markov decision process: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal T, r>$.
- Partially observed MDP: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal O, \mathcal T, \varepsilon, r>$, where $\varepsilon$ stands for emission probability $p(o_t|s_t)$.

## 4.2. Definition of RL problem

- $\underbrace{p_\theta(s_1,a_1,\cdots,s_T,a_T)}_{p_\theta(\tau)}=p(s_1)\mathop\Pi\limits_{t=1}^T\underbrace{\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)}_{\text{Markov chain on (s,a)}}$.

  - Once $\pi_\theta$ is fixed, you can actually view the MDP as a <font color="red">*Markov chain*</font> on an augmented state space (original $\mathcal S$ and $\mathcal A$), $p((s_{t+1},a_{t+1})|(s_t,a_t))=p(s_{t+1}|s_t,a_t)\pi_\theta(a_{t+1}|s_{t+1})$.

- $\theta^*=\arg\max\limits_{\theta}E_{\tau\sim p_{\theta}(\tau)}\left[\sum\limits_{t=1}r(s_t,a_t)\right] $. (Goal)

  - $= \arg\max\limits_{\theta}\sum\limits_{t=1}^TE_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]$. (Markov Chain)
  - And thus $\begin{pmatrix}s_{t+k}\\a_{t+k}\end{pmatrix}=\mathcal T^k\begin{pmatrix}s_{t}\\a_{t}\end{pmatrix}$, if $T=\infty$, will $p(s_t,a_t)$ converge to a <font color="red">*stationary*</font> distribution? (just solve $\mu=\mathcal T \mu$)?
    - Yes, under ergodicity.
    - And the expected return under infinite horizon is **dominated** by the stationary state, and thus the objective is $\arg\max\limits_{\theta}E_{(s,a)\sim p_\theta(s,a)}[r(s,a)]$.

- > Even if you take expectations of non-smooth, non-differentiable nasty horrible functions, they can actually be reasonably behaved in the parameters of the distribution under with you take expectation. $\leftarrow$ why GD can work in RL.

## 4.3. Anatomy of a RL algorithm

- ![image-20201105104915804](./pic/image-20201105104915804.png)

- How do we deal with *expectations*?
  -  ![image-20201105110020150](./pic/image-20201105110020150.png)
  - <font color="red">But actually it's not that easy to modify $\pi_\theta$, because $Q$ is also dependent on $\pi_\theta$, but this is the intuition of $Q$.</font>
- The value function and the Q-function:
  - <img src="./pic/image-20201105110349477.png" alt="image-20201105110349477" style="zoom: 50%;" />
  - Idea 1: if we know $Q_{\pi_\theta}$, we can improve $\pi$ greedily.
  - Idea 2: if $Q_{\pi_\theta}(s,a)>V_{\pi_\theta}(s)$, then $a$ is **better than average**. 

## 4.4. Brief overview of RL algorithm types

- ![image-20201105110941580](./pic/image-20201105110941580.png)
  - AC is the combination of PG and value-based (it use value function in PG to improve policy).
- Consideration for algorithm selection:
  - Sample efficiency.
  - Stability (convergence) and ease use.
  - Assumptions (observability, episodic learning, continuity or smoothness)
- ![image-20201105111439799](./pic/image-20201105111439799.png)

# Lecture 05

- Evaluate the objective: ![image-20201107093448703](./pic/image-20201107093448703.png)

## 5.1. Policy Gradient

- ![image-20201107093649374](./pic/image-20201107093649374.png)
  - ![image-20201107094007532](./pic/image-20201107094007532.png)
  - **Why** is the reward independent of $\theta$?
    - If the policy change, the expectation of reward will change, which is accounted for at the front term, but once you sampled $s_t,a_t$ from your policy, the reward will only depend on $s_t,a_t$.

- Basic components of RL algorithm (PG):
  - <font color="orange">generate samples: just run the policy.</font>
  - <font color="gren">fit a model to estimate return: just run the policy.</font>
  - <font color="blue">improve the policy: $\theta=\theta+\alpha\nabla_\theta J(\theta)$</font>
- PG via MLE:
  - $\nabla_\theta log\,\pi_\theta(\tau)$ actually points at the direction that increase $\pi_\theta(\tau)$ (the probability of $\tau$).
  - ![image-20201107095427620](./pic/image-20201107095427620.png)
    - PG only increase the probability of good trajectories, while MLE increase the probability of all trajectories.
    - <font color="grey">BTW, we might use the built-in MLE-gradient to compute the gradient of PG.</font>
- <font color="red">Why PG won't work?: *the gradient* has high variance!</font>
  - Assume we are trying to tuning a Gaussian policy, and receive three samples, one negative one the left and two positive one the right.
  - ![image-20201107100459679](./pic/image-20201107100459679.png)
  - **After adding a big constant to all samples**
  - ![image-20201107100539826](./pic/image-20201107100539826.png)
  - **What if the two "good" rewards are zero**?
    - You policy (center of the Gaussian) will shift either left or right depending on the previous position of your policy. (either coming from the left or the right).

## 5.2. Basic variance reduction: causality

- The action you choose now will **not** affect the rewards you received in the past.

  - ![image-20201107101106405](./pic/image-20201107101106405.png)

  - This is a still unbiased estimation of PG, but with lower variance. (see paper for proof)

## 5.3. Basic variance reduction: baselines

- We only want the trajectories which are "better" than average to be more often.
  - ![image-20201107101933959](./pic/image-20201107101933959.png)
  - Still unbiased! (But the expected return is not actually the optimal choice of baseline.)
-  Analyzing variance for optimal *b*:
  - ![image-20201107104958617](./pic/image-20201107104958617.png)
  - 最优的baseline是不同$r(\tau)$基于$g(\tau)^2$ (gradient magnitude)的加权平均. (不要考虑$\nabla_\theta\,log\pi_\theta(\tau)$是向量的情况, 因为那样的话我们就不能仅仅优化Var了,因为Var是矩阵了,优化一个矩阵是一个不明确的操作.)

## 5.4. On-policy via Off-policy

- Importance Sampling (IS):
  - ![image-20201107105515950](./pic/image-20201107105515950.png)
  - **需要满足条件**: $p(x)f(x)$ is always 0 whenever $q(x)=0$.
- Problem: the important coefficient will increase the variance:
  - Consider *causality*:
    - ![image-20201107110753092](/home/lemon/Workspace/myCheatSheet/Learn/UCB/pic/image-20201107110753092.png)
  - A little modification to get the **policy iteration** algorithm.
    - Note that you will get a different gradient after this modification, but the new gradient will still improve your policy, but for a different reason.
    - ![image-20201107110941927](/home/lemon/Workspace/myCheatSheet/Learn/UCB/pic/image-20201107110941927.png)

- A first-order approximation of IS. (**NOTE**: using **state-action** distribution instead of **trajectory** can change $\Pi$ to $\sum$)
  - ![image-20201107111350323](/home/lemon/Workspace/myCheatSheet/Learn/UCB/pic/image-20201107111350323.png)