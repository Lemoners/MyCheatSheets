# Lecture 01

# Lecture 04

## 4.1. Definition of MDP 

- Markov chain: $\mathcal{M}=<\mathcal S, \mathcal T>$. (state space and transition operator).
  - Let $\mu_{t,i}=p(s_t=i)$, $\overrightarrow \mu_{t+1}=\mathcal T \overrightarrow \mu_t$
- Markov decision process: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal T, r>$.
- Partially observed MDP: $\mathcal{M}=<\mathcal S, \mathcal A, \mathcal O, \mathcal T, \varepsilon, r>$, where $\varepsilon$ stands for emission probability $p(o_t|s_t)$.

## 4.2. Definition of RL problem

- $\underbrace{p_\theta(s_1,a_1,\cdots,s_T,a_T)}_{p_\theta(\tau)}=p(s_1)\mathop\Pi\limits_{t=1}^T\underbrace{\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)}_{\text{Markov chain on (s,a)}}$.

  - Once $\pi_\theta$ is fixed, you can actually view the MDP as a <font color="red">*Markov chain*</font> on an augmented state space (original $\mathcal S$ and $\mathcal A$), $p((s_{t+1},a_{t+1})|(s_t,a_t))=p(s_{t+1}|s_t,a_t)\pi_\theta(a_{t+1}|s_{t+1})$.

- $\theta^*=\arg\max\limits_{\theta}E_{\tau\sim p_{\theta}(\tau)}\left[\sum\limits_{t=1}r(s_t,a_t)\right] $. (Goal)

  - $= \arg\max\limits_{\theta}\sum\limits_{t=1}^TE_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]$. (Markov Chain)
  - And thus $\begin{pmatrix}s_{t+k}\\a_{t+k}\end{pmatrix}=\mathcal T^k\begin{pmatrix}s_{t}\\a_{t}\end{pmatrix}$, if $T=\infty$, will $p(s_t,a_t)$ converge to a <font color="red">*stationary*</font> distribution? (just solve $\mu=\mathcal T \mu$)?
    - Yes, under ergodicity.
    - And the expected return under infinite horizon is **dominated** by the stationary state, and thus the objective is $\arg\max\limits_{\theta}E_{(s,a)\sim p_\theta(s,a)}[r(s,a)]$.

- > Even if you take expectations of non-smooth, non-differentiable nasty horrible functions, they can actually be reasonably behaved in the parameters of the distribution under with you take expectation. $\leftarrow$ why GD can work in RL.

## 4.3. Anatomy of a RL algorithm

- <img src=".\pic\image-20210227163959923.png" alt="image-20210227163959923" style="zoom:50%;" />
- Value-based algorithms don't have a essential policy ($\arg\max$ if any).
- How do we deal with *expectations*?
  -  ![image-20201105110020150](./pic/image-20201105110020150.png)
  - <font color="red">But actually it's not that easy to modify $\pi_\theta$, because $Q$ is also dependent on $\pi_\theta$, but this is the intuition of $Q$.</font>
- The value function and the Q-function:
  - <img src=".\pic\image-20210227164050358.png" alt="image-20210227164050358" style="zoom:67%;" />
  - Idea 1: if we know $Q_{\pi_\theta}$, we can improve $\pi$ greedily.
  - Idea 2: if $Q_{\pi_\theta}(s,a)>V_{\pi_\theta}(s)$, then $a$ is **better than average**. 

## 4.4. Brief overview of RL algorithm types

- <img src="./pic/image-20201105110941580.png" alt="image-20201105110941580" style="zoom:35%;" />
  - AC is the combination of PG and value-based (it use value function in PG to improve policy).
- **Consideration** for algorithm selection:
  - Sample efficiency.
  - Stability (convergence) and ease use.
  - Assumptions (observability, episodic learning, continuity or smoothness)
- ![image-20201105111439799](./pic/image-20201105111439799.png)

# Lecture 05

- Evaluate the objective: <img src=".\pic\image-20210227164157819.png" alt="image-20210227164157819" style="zoom: 50%;" />

## 5.1. Policy Gradient

- <img src=".\pic\image-20201107093649374.png" alt="image-20210223103332006" style="zoom:50%;" />
  - <img src=".\pic\image-20210227164308580.png" alt="image-20210227164308580" style="zoom:50%;" />
  - **Why** is the reward independent of $\theta$?
    - If the policy change, the expectation of reward will change, which is accounted for at the front term, but once you sampled $s_t,a_t$ from your policy, the reward will only depend on $s_t,a_t$.
- Basic components of RL algorithm (PG) / *REINFORCE* algorithm:
  - <font color="orange">generate samples: just run the policy.</font>
  - <font color="gren">fit a model to estimate return: just run the policy.</font>
  - <font color="blue">improve the policy: $\theta=\theta+\alpha\nabla_\theta J(\theta)$</font>
- PG vs. MLE:
  - $\nabla_\theta log\,\pi_\theta(\tau)$ actually points at the direction that increase $\pi_\theta(\tau)$ (the probability of $\tau$).
  - ![image-20201107095427620](./pic/image-20201107095427620.png)
    - PG only increase the probability of **good** trajectories, while MLE increase the probability of all trajectories. So PG is like *a weighted version* of MLE.
    - <font color="grey">BTW, we might use the built-in MLE-gradient to compute the gradient of PG.</font>
- Markov Properties are **NOT** actually used in PG, so we can deduct the “exact” derivatives at **Partial Observation**:
  - <img src=".\pic\image-20210223104403663.png" alt="image-20210223104403663" style="zoom: 50%;" />
- <font color="red">Why PG won't work?: *the gradient* has high variance!</font>
  - Assume we are trying to tuning a Gaussian policy, and receive three samples, one negative one the left and two positive one the right. $\rightarrow$: penalty(left); encourage(right)
  - <img src="./pic/image-20201107100459679.png" alt="image-20201107100459679" style="zoom: 25%;" />
  - **After adding a big constant to all samples**  $\rightarrow$: encourage(left); encourage(right), so the policy won't shift that much to the right compared to the above one.
  - <img src="./pic/image-20201107100539826.png" alt="image-20201107100539826" style="zoom:25%;" />
  - **What if the two "good" rewards are zero**?
    - You policy (center of the Gaussian) will shift either left or right depending on the previous position of your policy. (either coming from the left or the right).

## 5.2. Basic variance reduction: causality

- *Causality*: The action you choose **now** will **not** affect the rewards you received in the **past**. $\neq$ Markov properties.
  - <img src="./pic/image-20201107101106405.png" alt="image-20201107101106405" style="zoom:30%;" />
- This is a still **unbiased estimation** of PG, but with **more** variance. (see paper for proof)
  - $\sum\limits_{i=1}^N\sum\limits_{t=1}^{T}\nabla_{\theta}log\pi_\theta(a_{i,t}|s_{i,t})(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'})) = 0$
    - For that $E_{\tau\sim p(\theta)}[\nabla_\theta log\, \pi_\theta(\tau)\cdot b] = 0$, as long as $b$ is constant w.r.t. $\theta$ and $\tau$
  - $原式 = \sum\limits_{t=1}^T\mathbb{E}_{s_t,a_t}[\nabla_\theta log\pi_\theta(a_t|s_t)(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'}))]$, and $(\sum\limits_{t'=1}^{t-1} r(s_{i,t'},a_{i,t'}))$ is constant w.r.t. $\theta$ and $a_t$.

## 5.3. Basic variance reduction: baselines

- We only want the trajectories which are "better" than average to be more often.
  - ![image-20201107101933959](./pic/image-20201107101933959.png)
  - Still **unbiased**, but will alter the variance.
-  Analyzing variance for acquiring optimal *b*:
  - ![image-20201107104958617](./pic/image-20201107104958617.png)
  - The second term of the variance is independent of $b$, so we only need to focus on the former one.
  - 结论: 最优的baseline是不同$r(\tau)$基于$g(\tau)^2$ (gradient magnitude)的**加权平均**.
    - 注: 不要直接考虑$\nabla_\theta\,log\pi_\theta(\tau)$是向量的情况, 因为那样的话我们就不能仅仅优化Var了,因为Var是矩阵了,优化一个矩阵是一个不明确的操作.
    - 如果$\theta$是向量, 那你可以对它的每一个维度进行上述计算, 所以最后对于$\theta$不同的维度会有不同的optimal $b$.

## 5.4. On-policy via Off-policy

- Importance Sampling (<font color="red">**IS**</font>):
  - <img src="./pic/image-20201107105515950.png" alt="image-20201107105515950" style="zoom:20%;" />
  - **需要满足条件**: $p(x)f(x)$ is always 0 whenever $q(x)=0$.
  
- Problem: the important coefficient will increase the variance:
  - Consider *causality*: (NOTE: $\pi_\theta(\tau)=p(s_1) \mathop\Pi\limits_{t=1}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$)
    - <img src="./pic/image-20201107110753092.png" alt="image-20201107110753092" style="zoom:140%;" />
    - Causality is **NOT** an *arbitrary* modification, it's equal to its previous formulation.
  - Ignoring the latter [product](乘积项) will result in the **policy iteration** algorithm.
    - Note that you will get a *different gradient* after this modification, but the new gradient **will still improve** your policy, but for a different reason.
    - <img src="./pic/image-20201107110941927.png" alt="image-20201107110941927" style="zoom:33%;" />
  
- A first-order approximation of IS.
  
  - <img src="./pic/image-20201107111350323.png" alt="image-20201107111350323" style="zoom: 33%;" />
  - The importance coefficient is exponential in $T$, and we can assume it's smaller/bigger than $1$, so the product will decrease to zero / increase to infinity essentially fast ($\Leftrightarrow$ the variance will increase to infinity), which is troublesome.
  - **How to write the objective differently**: so instead of importance sampling over **entire trajectories**, you can just consider the importance sample over the **state-action joints**. 
  - **Ignore the state portion**: when ignoring the state marginal distribution, you actually didn't get the right gradient, but this will only result in bounded error when $\theta'$ is not too far from $\theta$.

## 5.5. Implementing Policy Gradients

- <img src=".\pic\image-20210223113353534.png" alt="image-20210223113353534" style="zoom: 50%;" />
  - Assume $\theta\in\mathbb{R}^n$ ($n$ can be very high for NN), then direct calculation will require $N\times T\times n$ times.
  - We want to utilize the back propagation. $\rightarrow$ we need a **graph** such that its gradient is the policy gradient.
- ???: YouTube上的代码似乎是一种自己根据REINFORCE定义的东西: $\nabla\mathcal{L}(a_\theta, a_{\theta'})q(a_\theta, a)$

## 5.6. Advanced Policy Gradient

- Numerical issue with PG:
  - Different **gradient magnitude** at different parameters. So the gradient might be dominant by some parameter. e.g. $\nabla_\theta J(\theta)=\begin{bmatrix}1000000\\1\end{bmatrix}$
  - Some parameter **affect the policy** a lot more than others. e.g. $\pi_\theta(s)=\theta_1^{10}s_1+\theta_2s_2$, it's hard to pick one *step size* for all parameters.
- **Covariant/natural Policy Gradient**:
  - The constrained optimization view of PG:
    - <img src=".\pic\image-20210223151427891.png" alt="image-20210223151427891" style="zoom: 50%;" /> $\rightarrow$ this's actually maximize the first order Taylor expansion of $J(\theta')$ at $\theta$.
  - We want the step to be the same in **policy space** rather than in **parameter space**.
    - <img src=".\pic\image-20210227164520319.png" alt="image-20210227164520319" style="zoom: 50%;" />
    - We usually use KL-divergence, but to simplify, we use the second order Taylor expansion as surrogate of KL-divergence.
      - <img src=".\pic\image-20210223151816996.png" alt="image-20210223151816996" style="zoom:50%;" />
      - The Fisher-information matrix can be estimated through samples.
    - Finally: <img src=".\pic\image-20210223151925042.png" alt="image-20210223151925042" style="zoom:50%;" />
      - And the solution is <img src="C:\Users\98748\AppData\Roaming\Typora\typora-user-images\image-20210223151953989.png" alt="image-20210223151953989" style="zoom: 50%;" />, where $\alpha$ is your Lagrange multiplier.
  - Related works:
    - <img src=".\pic\image-20210223152057335.png" alt="image-20210223152057335" style="zoom:35%;" />

# Lecture 06 Actor-critic Algorithms

- The key idea for this lecture is to find *better estimate* of $\hat Q(s,a)$.

## 6.1. Improving the policy gradient with a critic

- Due to the stochasticity of RL, a better **"reward to go"** can be an **average** over possible trajectories; and this also bring less variance than single trajectory sample. $Var(\bar X)=\frac{1}{n}Var(x_i)$
  - <img src="./pic/image-20201121130053471.png" alt="image-20201121130053471" style="zoom:30%;" />
- Subtract a **baseline** which denotes `average reward`. 
  - Average of what? 
    - Depend on the Q-value: $\frac{1}{N}\sum\limits_{i}Q(s_{i,t},a_{i,t})$ 
    - Depend on the state: $V(s_t)=E_{a_t\sim\pi_\theta(a_t|s_t)}[Q(s_t,a_t)]$)
  - The **advantage function**$A(s_{i,t},a_{i,t}) = Q(s_{i,t},a_{i,t})-V(s_{i,t})$: 
    - How better this action is than the average action.
    - <img src="./pic/image-20201121130602834.png" alt="image-20201121130602834" style="zoom: 25%;" />
      - Some time in algorithm, the advantage function might be biased, but it's worthy because of the enormous variance reduction.
    - <img src="./pic/image-20201121130707261.png" alt="image-20201121130707261" style="zoom:25%;" /> 

## 6.2. The policy evaluation problem

- Monte Carlo policy evaluation:
  - Traditional MC: It's a little hard to calculate $\sum\limits_i\sum\limits_{t'=t}^Tr(s_{i,t'},a_{i,t'})$ because this requires to reset the simulator to the exact state $s_{t'}$.
  - MC evaluation with **function approximation** (Neural Network)
    - $V^{\pi}(s_t)\approx\sum\limits_{t'=t}^Tr(s_{t'},a_{t'})$. => <img src=".\pic\image-20210227172702751.png" alt="image-20210227172702751" style="zoom: 50%;" />
    - It's not good as $\sum\limits_i\sum\limits_{t'=t}^Tr(s_{i,t'},a_{i,t'})\Rightarrow$ "sample multiple trajectory $s_t$", but still good estimation.
    - <img src=".\pic\image-20210227172421018.png" alt="image-20210227172421018" style="zoom:50%;" />: 
      - Also, even if you can't visit the same state, but as you visit nearby state, the function approximator will learn to know that nearby states probably have similar rollouts => less variance in prediction.
- Improve the *target* used in MC evaluation: 
  - <img src="./pic/image-20201121150132050.png" alt="image-20201121150132050" style="zoom:25%;" />
  - The ideal target is a slightly incorrect because of $\bar V_\phi^\pi(s_{i,{t+1}})$ can be noisy, so at initial state it's better to use MC target. But the ideal target has lower variance (especially useful when the environment is very stochastic). 
  - Using ideal target is referred to a `bootstrapped` estimate.

- An AC algorithm:
  - <img src="./pic/image-20201121150657423.png" alt="image-20201121150657423" style="zoom:25%;" />

## 6.3. Discount factors

- When $T\rightarrow\infty$, $\bar{V}^{\pi}_{\phi}$ can get infinitely large in many cases. Simple trick. use $\gamma\in[0,1], e.g.\,0.99$, the philosophy is: `better to get rewards sooner than later`.
  
  - Why: 1) in most ways, we care more about the present reward; 2) the agent might "die" before receives future reward, we can use $\gamma$ to **model** it. 3) it's also a bias-variance trade off. ([see](#6.4))
    - $\gamma$ **changes** the MDP, the agent now has the probability $1-\gamma$ to die at every state.
      - <img src="./pic/image-20201121151040626.png" alt="image-20201121151040626" style="zoom:33%;" />
  
- PG with discount factors:
  - <img src=".\pic\image-20210227174540037.png" alt="image-20210227174540037" style="zoom: 40%;" /> 
  
  - The critic <img src=".\pic\image-20210227174754103.png" alt="image-20210227174754103" style="zoom: 70%;" /> is actually **equals to** option 1:
    
    - $\hat{V}_\phi^\pi(s_{t_0})=\sum\limits_{t=t_0}^T\gamma^{t-t_0}r(s_t,a_t)=r(s_{t_0},a_{t_0})+\gamma\hat{V}_\phi^\pi(s_{t_0+1})$
    
  - And if we change the form of option 2 using causality trick:
    
    - <img src=".\pic\image-20210227175101537.png" alt="image-20210227175101537" style="zoom: 50%;" />
    - <img src="./pic/image-20201121151740629.png" alt="image-20201121151740629" style="zoom: 40%;" />
    - Because of the discount, not only we care less about the *reward* in the future, but also care less about the *decision* in the future. (`Making right decisions is more important at earlier stage.`)
    
  - Option 1 is we frequently use, while option 2 is **the right solution for** dealing with the MDP where you actually have probability $1-\gamma$ to die at every state *or* the MDP with discounted objective $E_{\tau\sim p_\theta(\tau)}[\sum_t\gamma^t r(s_t,a_t)]$.
    
    - In option 1, $\gamma$ can be viewed as a way to eliminate variance in your value estimator.
    
    - Further reading:
    
      - > Bias in natural actor-critic algorithms. ICML 2014.
  
- **Online** actor-critic algorithm:

  - <img src=".\pic\image-20210227180011842.png" alt="image-20210227180011842" style="zoom:50%;" />

## 6.4. The actor-critic design decisions

- Architecture design:
  - ![image-20201121152709737](./pic/image-20201121152709737.png)
    - In shared network design, the **shared layers** get gradients both from *policy* and *value function*. So you have to carefully balance two different gradient (and their magnitudes) so that one won't be blown off by the other.
  
- Batch-size:

  - One way to minimize variances.
  - The state/action pair in batches shouldn't be too correlated. So a better choice for online RL is to use *parallel workers*:
    - <img src=".\pic\image-20210228084858019.png" alt="image-20210228084858019" style="zoom:50%;" />
  - `Remark`: In asynchronous AC, one problem is some agents might use older $\theta$ even if $\theta$ has been updated, but AC is an **on-policy** algorithm.

- **Critic as baselines**:

  - ![image-20201121153314978](./pic/image-20201121153314978.png)
    - The baseline can depend only on *state*, while it's expectation still works out to zero. (可以仿照之前证明b在期望下为零证明, 思路基本一致, 做对于$(s,a)$ pair的期望.)
    
  - **Action-dependent baselines**:
- ![image-20210228090737613](.\pic\image-20210228090737613.png)
    - Action-dependent baselines don't integrate to zero but an **error term**, so you have to account for the error term.
    - `Why bother using this method?`: sometimes the second term can be estimated very accurately, because the estimation doesn't involve in new states, so you don't need to interact with the environment and thus you can generate a huge number of samples. Also, if you carefully choose the model for your policy and Q-function, the second term might have an analytical solution.
      - So the variance of the *total equation* will be small.
    - Further reading : Gu et al. 2016 (Q-prop)
  
- **Eligibility traces & n-step returns**:
  - ![image-20201121153824244](./pic/image-20201121153824244.png)
  - We want to combine these two, and use $\hat{A}^\pi_{MC}$ (Monte Carlo)in the short term and $\hat{A}^\pi_{C}$ (Critic) in the long term:
    - $\hat{A}^\pi_{MC}$ will have high variance in the future, because it's single-sample estimate.
    -  $\hat{A}^\pi_{C}$ will have less bias in the future, because when using discount factor, your reward usually converges to zero.
    - So one way is to **cut off the tails** in $\hat{A}^\pi_{MC}$, so we won't be affected by the high variance caused by the infinite future, and use critic $\gamma^n\bar{V}_\phi^\pi(s_{t+n})$  to **compensate** for it. (在这里$\sum\limits_{t'=t}^{t+n}$表示$t'\in[t,t+n$<font color="red">$)$</font>$\Rightarrow$n-step).
      - <img src="./pic/image-20201121154042679.png" alt="image-20201121154042679" style="zoom: 25%;" />
      - <img src="./pic/image-20201121154229642.png" alt="image-20201121154229642" style="zoom: 33%;" />
  
- **GAE**:
  - *What if we want to choose multiple n*?
  - ![image-20210228092953108](.\pic\image-20210228092953108.png)
  
  - <span id='6.4'>So implement discount alone can be interpreted as variance/bias trade-off: because for smaller $\gamma$, it's eliminate the MC-estimation with large *n*, which has large variance, but small bias. </span>

# Lecture 07 Value Function Methods

## 7.1. Value function

- **Policy iteration**: = policy evaluation + policy improvement
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302114602208.png" alt="image-20210302114602208" style="zoom: 33%;" />

- Can we **omit** Policy Gradient completely?
  - Yes: $A^\pi(s_t,a_t)$ denotes how much better is $a_t$ than the average action according to $\pi$ at $s_t$. And $\arg\max\limits_{a_t}A^\pi(s_t,a_t)$ is actually the best action at $s_t$ if we then follow $\pi$. As we actually know **how to act** if we have $A^\pi(s_t,a_t)$, we don't need an explicit policy. And we know **how to improve** it by setting $\pi'(a_t|s_t)=\{\begin{aligned}&1 \text{ if } a_t = \arg\max\limits_{a_t}A^\pi(s_t,a_t)\\&0 \text{ else} \end{aligned}\ $, this will improve your policy for any $\pi$.
- Evaluation of $A^\pi(s,a)$ in Policy Iteration:
  - **Dynamic Programming:**
    - Assume: $p(s'|s,a)$ is known, $S,A$ are small and discrete.
    - E.g. <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302115459207.png" alt="image-20210302115459207" style="zoom:67%;" />
  - Simpler DP with **Q-function**:
    - $A^\pi(s,a)=r(s,a)+\gamma E_{s'}[V^\pi(s')]-V^\pi(s)\approx r(s,a)+\gamma E_{s'}[V^\pi(s')] = Q^\pi(s,a)$.
    - The policy is to find $\arg\max A^\pi(s,a)=\arg\max Q^\pi(s,a)$, and to update $V$, $V^\pi(s)=E_{a\sim\pi(a|s)}[r(s,a)+\gamma E_{s'}V(s')]=_{a=\arg\max Q(s,a)}Q(s,a)$.
      - So we can skip this step by just updating $V$ with the value of $\max Q^\pi(s,a)$ and we derive *value iteration*.
    - **Value Iteration**:
      -  <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302120832554.png" alt="image-20210302120832554" style="zoom: 50%;" />
      - `Remark`: 
        -  *Value iteration*中的$E_{s'\sim(s'|s,\pi(s))}$也可以不用知道Transition dynamics, 而是直接使用sample做估计. **但是DP是需要transition dynamic**的.
        -  *Value iteration*中利用了$Q$和$V$(的关系)在做迭代 <font color="black">$=$</font> 用$Q$或$V$自己的*Bellman-equation*做*Bootstrap*迭代.
        -  但是`value iteration` i.e. $V^\pi(s)\rightarrow$<font color="red">$\max\limits_a$</font>$\{r(s,a)+\gamma E_{s'}[V^\pi(s')])\}$. $\neq$ `policy evaluation` : i.e. $V^\pi(s)\rightarrow r(s,\pi(s))+\gamma E_{s'}[V^\pi(s')])$.
        -  前者在更新`value`的同时在更新`policy`, 后者仅仅是根据`policy`在做估计.
        -  `value iteration`就是对于`argmax` policy 的`policy evaluation`, 因为这个*greedy*的policy可以保证提升.

## 7.2. Fitted Value Iteration & Q-Iteration

> **`Fitted` means you use a function approximator to *fit* the value function.**

- Fitted value iteration: "value iteration with function approximator" <font color="red">$\Leftarrow$ Requre to know the transition dynamics! $E(V(s'))$</font>
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302162845682.png" alt="image-20210302162845682" style="zoom:50%;" />$\Leftarrow$ Supervised learning for $V_\phi$
  - When we don't have the **transition dynamics**:
    - Policy evaluation:
      - $V^\pi(s)\rightarrow r(s,\pi(s))+\gamma E_{s'\sim p(s'|s,\pi(s))}[V^\pi(s')]$ $\Leftrightarrow$ $Q^\pi(s,a)\rightarrow r(s,a)+\gamma E_{s'\sim p(s'|s,a)}[Q^\pi(s', \pi(s'))]$.
      - The difference between these two might sound subtle, but it's very important! Because the sample of $s'$ **doesn't depend on the policy**, so we can reuse the $(s,a,s')$ pair even when $\pi$ has changed. 
      - Both of them **doesn't require knowing the transition dynamics**, you can just use samples for estimation. 
  - **Fitted $Q$ iteration algorithm**:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302164929088.png" alt="image-20210302164929088" style="zoom: 50%;" />
      - Because the policy $\pi$ is greedy, so:
        - $Q^\pi(s,a)\rightarrow r(s,a)+\gamma E_{s'\sim p(s'|s,a)}[Q^\pi(s', \pi(s'))])\rightarrow r(s,a)+\gamma E_{s'\sim p(s'|s,a)}[\max_{a'}Q^\pi(s', a')] \xrightarrow{\text{sampling}} r(s,a)+\gamma \max_{a'}Q^\pi(s', a')$
      - Works for off-policy:
        - Because $r(s_i,a_i)+\gamma \max_{a'}Q_\phi(s_i',a_i')$ doesn't depend on the policy $\pi$. (one interpretation is our policy is $\arg\max$, so our policy + $Q$ equals to $\max Q$, which offset $\pi$.)
        - So you just need $(s,s',a)$ pair to improve your $Q$ while you don't really care where do they come from.
    - General form:
      - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302165507692.png" alt="image-20210302165507692" style="zoom:50%;" />

## 7.3. From Q-iteration to Q-learning

- What's fitted Q-iteration optimizing:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302194817620.png" alt="image-20210302194817620" style="zoom:50%;" />
- Online Q-learning algorithms:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210302194958508.png" alt="image-20210302194958508" style="zoom: 50%;" /> $\Leftarrow$ policy is $\arg\max$, which is <font color="red">*greedy*</font>.
    - You take **one** action, and perform **one** update.
  - Exploration with Q-learning: $\epsilon$-*greedy*; $\pi(a_t|s_t)\propto \exp(Q_\phi(s_t,a_t))$.

## 7.4. Value functions in theory

- Bellman Operator $\mathcal B$:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303093303199.png" alt="image-20210303093303199" style="zoom:50%;" />
  - `NOTE`:
    - $V$ is a vector, which stands for $\begin{bmatrix}V(s_1)\\\cdots\\V(s_n)\end{bmatrix}$.
    - $V^*$ always exists, always unique, always corresponds to the optimal policy.
  - Convergence:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303093621762.png" alt="image-20210303093621762" style="zoom: 50%;" />
- **Non-tabular** value function learning:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303094016131.png" alt="image-20210303094016131" style="zoom:50%;" />
  - Where $\Omega$ is all possible value function in the approximator space (e.g. NN).
  - The general procedure is: $V\rightarrow \mathcal BV\rightarrow$ find the nearest $V'\in\Omega$ to $\mathcal BV$. (projection).
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303094240440.png" alt="image-20210303094240440" style="zoom:33%;" />
    - Define a new operator $\Pi:\Pi V=\arg\min_{V'\in\Omega}\frac{1}{2}\sum||V'(s)-V(s)||^2$, and $\Pi$ is a *projection* onto $\Omega$ in terms of $l_2$ norm. And it's a contraction, because when you project two vector to a line, they can't get further, they will normally get closer. 
  - `Remark`:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303094600945.png" alt="image-20210303094600945" style="zoom:50%;" />
    - $\mathcal B$ is contraction w.r.t. $l_{\infty}$, $\Pi$ is contraction w.r.t. $l_2$; but **$\Pi\mathcal B$** is **NOT** contraction of any kind.
  - Fitted **Q**-iteration suffers the same limitation: no general convergence guarantee.
    - Q-learning is <font color="red">**NOT**</font> gradient descent of any well-defined objective:
      - Because you don't consider the gradient of the target value. (Even if you do, which is *the residual algorithm*, it has very poor numerical properties.)
      - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303095524132.png" alt="image-20210303095524132" style="zoom: 50%;" />
- The fitted **bootstrapped policy evaluation** in batch AC doesn't generally converge:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210303101931872.png" alt="image-20210303101931872" style="zoom:50%;" />

# Lecture 08 Deep RL with Q-Functions

## 8.1. Correlated samples in online Q-learning

- Problem: ![image-20210306164027585](D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210306164027585.png)
  - *Sequential states* (local highly correlated window) may cause your Q-function to always overfit to the current seen trajectory.
    - Countermoves: synchronized/asynchronized parallel Q-learning....
  - *Changing target value*: it's like chasing its own tail. The target changes every time after you apply the gradient update, so it's hard for $Q_\phi$ to converge.
    - Countermoves: target network....
- **Relay buffers**:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210306165024361.png" alt="image-20210306165024361" style="zoom:80%;" />
  - We also need some strategy to sample trajectories for feeding the replay buffer.
  - $K=1$ is common, but larger $K$ is more efficient

## 8.2. Target Networks

- Q-learning with replay buffer and target networks:
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210306165828500.png" alt="image-20210306165828500" style="zoom:67%;" />
- "Classical" deep Q-learning algorithm:
  - Same as the above with $K=1$.
- Alternative target network:
  - `Different lay`: right after you update $\phi'$, the lag between $\phi$ and $\phi'$ is going to be big. But when $\phi'$ has been fixed for a number of iterations, the lag between $\phi$ and $\phi'$ is going to be small. So one intuition is, can we want the lag between $\phi$ and $\phi'$ to be roughly the same during iteration?
  - **Updating $\phi'\leftarrow\tau\phi'+(1-\tau)\phi$** every time after you update $\phi$. ($N=1$).
    - $\tau=0.999$ works well. However, mixing NN parameters is tricky. (Linear mixing of nonlinear function parameters)
    - Further reading: Polyak averaging.

## 8.3. A General View of Q-learning

<img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210306172444177.png" alt="image-20210306172444177" style="zoom:67%;" />

- Different Q-learning algorithms are changing the speed of different processes.

## 8.4. Improving Q-learning

- Overestimation in Q-learning
  - $Q_\phi(s,a)=r(s,a)+\max_{a'}\gamma Q_{\phi’}(s',a')$
    - Your Q-function is not perfect, it's noisy. And the **argmax** will always select the **positive noises**, which results in *overestimation.*
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308201226624.png" alt="image-20210308201226624" style="zoom: 50%;" />
    - Intuitively, if we can decorrelate the noise in the *action selection ($\arg\max $)* and the noise in the Q*-value*, the problem can go away $\Rightarrow$ double Q-learning
    - Luckily, we already have double networks in Q-learning, so we don't need to introduce another network, instead we can use $\phi$ and $\phi'$ for value estimation and action selection separately:
      - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308201704056.png" alt="image-20210308201704056" style="zoom:50%;" />; but the decorrelation is definitely not always perfect, because $\phi\approx\phi'$ sometimes. 
- **Multi-step returns**:
  - Q-learning target: $y_t=r_t+\gamma\max_{a_{t+1}}Q_{\phi'}(s_{t+1},a_{t+1})$
    - If your Q-function is pretty bad, the target can only depend on the instant reward $r_t$, while $Q_{\phi'}(s_{t+1},a_{t+1})$ only contributes to the **noise**. $\Rightarrow$ At initial stage, Q-function is very biased, and your update will thus be awful.
  - The alternative is the *MC-target*, which is the sum of discounted reward, it has high variance (single sample estimation), but it's unbiased.
  - **Multi-step target/N-step return estimator:** <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308202640816.png" alt="image-20210308202640816" style="zoom:50%;" />
  - Q-learning with N-step returns:
    - It's a less biased target especially when Q-values are incorrect, and thus results in faster learning during the early stage.
    - **Can only be applied to <font color="red">on-policy</font> learning**! (or else you need to add off-policy modification factor)
      - $\Rightarrow$ because the MC-target needs to be sampled based on current policy <font color="red">$\pi$</font> as it's a target for $Q_{\pi}$.
      - On-policy: we need transitions $s_{t'},a_{t'},s_{t'+1}$ from $t$ to $t+N-1$.
    - Further reading: Safe and efficient off-policy reinforcement learning.

## 8.5. Q-learning with  Continuous Actions

- Problem with continuous action: $\arg\max$
  - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308203848003.png" alt="image-20210308203848003" style="zoom:40%;" />
- Potential solutions:
  - Stochastic optimization: 
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308204015847.png" alt="image-20210308204015847" style="zoom: 33%;" />
    - More accurate; iterative stochastic optimization: Cross-entropy method (CEM); CMA-ES
  - Use function class that is easy to optimize: e.g. *function class with closed form solutions for their optimal answers.* 
    - Quadratic functions. e.g. Normalized Advantage Functions:
      - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308204442731.png" alt="image-20210308204442731" style="zoom: 33%;" />
      - It's non-linear in the state, but it's quadratic in terms of the action given state.
  - Learn an approximate maximizer:
    - Train $\mu_\theta(s)$ such that $\mu_\theta(s)\approx\arg\max_a Q_\phi(s,a)$. You can also perceive $\mu_\theta(s)$ as some kind of *policy*.
      - Updating $\theta$: $\frac{dQ_\phi}{d\theta}=\frac{da}{d\theta}\frac{dQ_\phi}{da}$.
    - DDPG:
      - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308205139426.png" alt="image-20210308205139426" style="zoom:33%;" /> 

## 8.6. Implement Tips and Examples

- Simple practical tips for Q-learning:
  - Q-learning takes some care to stabilize: first task on easy, reliable task first, make sure your implementation is right (bugs/hyper parameters).
  - Large replay buffers help improve stability.
  - It takes time. [detail](it might be no better than random for a while, but it will get better once valuable transitions are found.)
  - Start with high $\epsilon$ (exploration) and decrease gradually. Schedule the learning rate (high to low), too. Adam optimizer can also help.
  - Bellman error gradients can be big; clip gradients or use Huber loss <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210308205634889.png" alt="image-20210308205634889" style="zoom:25%;" />.
    - For example there might be *huge negative reward* trajectories during early stage of exploration, the gradient of them might blow off your parameters. $\Rightarrow$ even if you don't care how bad it is, your Q-function will try to estimate exactly how bad it is if you don't clip the gradient.
  - Double Q-learning helps a lot.
  - N-step returns can help a lot during the early stage, but it has downsides since it's biased.
  - Run multiple random things. (OMG)

# Lecture 09 Advanced Policy Gradient

## 9.1. Recap of Policy Gradient (policy iteration)

- Why does policy gradient work?
  - What PG does can be decomposed into: 1) estimate $\hat A^\pi(s_t,a_t)$ for current policy $\pi$; 2) Use $\hat A^\pi(s_t,a_t)$ to get improved policy $\pi'$.
  - This is very similar to ***policy iteration*** (evaluate $A^\pi(s_t,a_t)$ + set $\pi\Leftarrow\pi'$), the biggest difference is that policy iteration using $\arg\max$ to update the policy, while PG performs a more *gentle* update.
- **Policy gradient** as **policy iteration**:
  - We want to prove: $J(\theta')-J(\theta)=E_{\tau\sim p_{\theta'}(\tau)}[\sum\limits_t\gamma^tA^{\pi_\theta}(s_t,a_t)]$.
    - The right hand is essentially what policy iteration does: it computes the advantage of **the old policy**, and uses it to find a new improved policy by $\theta'$.
    - If we can prove the *right hand* equals to the *left hand*, we can prove that `maximizing the expected value of the advantage of the old policy w.r.t. the trajectories of a new policy` is actually maximizing the RL objective. $\rightarrow$ Policy iteration does the right thing.
  - Proof:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210309194458112.png" alt="image-20210309194458112" style="zoom: 50%;" /> 
    - `Remark`
      - $E_{s_0\sim p(s_0)}[V^{\pi_\theta}(s_0)]=E_{\tau\sim p_{\theta'}(\tau)}[V^{\pi_\theta}(s_0)]$, because for any arbitrary $\theta$, $\tau\sim p_\theta(\tau)$ shares the **same** initial state distribution $s_0\sim p(s_0)$.
  - The connection between Policy iteration ($E_{\tau\sim p_{\theta'}(\tau)}[\sum\limits_t\gamma^tA^{\pi_\theta}(s_t,a_t)]$) and Policy gradients:
    - <img src="D:\Workspace\MyCheatSheets\Learn\UCB\pic\image-20210309195640935.png" alt="image-20210309195640935" style="zoom:67%;" />
    - The problem <font color="red">remains</font> is whether we can replace $E_{s_t\sim p_{\theta'}(s_t)}$ with $E_{s_t\sim p_\theta(s_t)}$, because in PG, we actually can't sample from the improved policy $\theta'$.
    - If we can perform the *replacement*, then we can derive that PG is actually improve the RL objective, too. $\rightarrow$ and the replacement can be executed ($p_\theta(s_t)\approx p_{\theta'}(s_t)$) when $\pi_\theta$ is closed to $\pi_{\theta'}$.