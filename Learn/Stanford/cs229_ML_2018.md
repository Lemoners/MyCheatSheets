Video: https://www.youtube.com/watch?v=rVfZHWTwXSA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU

Notes: http://cs229.stanford.edu/syllabus-autumn2018.html



# Lecture 3 - Locally Weighted & Logistic Regression

## 3.1. Linear Regression (Recap)

- $$
    \begin{aligned}
    (x^i,y^i)->&\text{i}th \text{ example}\\
    x^i \in \mathbb{R}^{n+1}&, y^i\in\mathbb{R}\\
    m = \#example&, y =\#features\\
    h_{\theta}(x)=\sum\limits_{j=0}^n \theta_jx_j &= \theta^Tx,  \text{where }x_0 \triangleq 1, \text{for that }x_0\theta_0 \triangleq b \\
    J(\theta) = \frac{1}{2}\sum\limits_{i=1}^m&(h_\theta(x^i)-y^i)^2\\
    \end{aligned}
    $$
    
- The professor use a fake feature $x_0 = 1$, which we usually refer to as the bias $b$.

## 3.2. Locally Weighted (Linear) Regression

- Parametric Learning algorithm:

    - Fix **fixed** set of parameters $\theta_i$ for data. e.g. Linear regression.

- Non-parametric Learning algorithm:

    - Amount of **data**/**parameters** you need to **keep** grows (linearly) with the size of data.

- Locally weighted regression:

    - (Fit $\theta$ with **data close to your prediction point**)

        - Maybe think about the case where you want to fit a non-linear with linear regression, it's more reasonable to only focus on the neighborhood.

    - $$
        \text{Fit }\theta\text{ to minimize}\\
        \sum\limits_{i=1}^m\omega^i(y^i-\theta^Tx^i)^2\\
        \text{where } \omega^i \text{ is a "weight" function}, \text{ a common choice is :}\\
        \omega^i = exp(-\frac{(x^i-x)^2}{2\tau^2})\\
        \text{where we called $\tau$: "bandwidth", when $\tau$ is small, you put more attention on the local area.}\\
        $$

- It's useful when you have tons of examples.

## 3.3. Probabilistic  Interpretation

- $$
    \text{Assume $y^i=\theta^Tx^i+\epsilon^i$, where $\epsilon^i$ stands for i.i.d. noise}\\
    \epsilon^i \sim N(0, \sigma^2)\\
\text{i.e. } (y^i|x^i; \theta) \sim N(\theta^Tx^i, \sigma^2);
    $$
    
    - Note: $p(y^i|x^i;\theta)$ means parameterized by $\theta$. $p(y^i|x^i,\theta)$ means conditional on $\theta$, but $\theta$ is **not** a random variable, so don't write like this.
    
- $\underbrace{\mathcal{L}(\theta)}_{\text{likelihood of $\theta$}} = p(y|x;\theta) = \mathop\Pi\limits_{i=1}^mp(y^i|x^i;\theta)$.

    -   Likelihood of the parameters <-> probability of the data. (Actually the same thing, but the 自变量(variable?) is different.)

- $l(\theta) = log \mathcal{L}(\theta) = \sum\limits_{i=1}^mlogp(y^i|x^i;\theta)$: Maximum likelihood estimation (MLE).

### Why least square errors $||y^i-\theta^Tx^i||^2$?

- $\arg\max\limits_\theta l(\theta) = \arg\min\limits_\theta \frac{1}{2}\sum\limits_{i=1}^m||y^i-\theta^Tx^i||^2 $. (Under the assumptions we made: `noise is Gaussian and i.i.d.`)
- <font color='red' size='6'>So linear regression actually has limitation on the dataset : noise to be gaussion and i.i.d</font>

## 3.4. Logistic Regression

### Classification

- $y\in\{0,1\}$: (binary classification)
    - Linear regression is **NOT** good for classification: <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200811123255695.png" alt="image-20200811123255695" style="zoom:25%;" />

### Logistic Regression

- $$
    h_\theta(x) \in [0,1]; h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}\\
    p(y=1 | x; \theta) = h_\theta(x);\; p(y=0 | x; \theta) = 1 - h_\theta(x); \\
    \text{i.e. } p(y|x;\theta) = h^y_\theta(x)(1-h_\theta(x))^{1-y}\\
    $$

    - Why sigmoid? Will explained at later lectures.
- If multi-class classification:
        - For each class, using logistic regression to prediction the probability of $x$ to be $y_i$, and using cross-entropy ($-P_{real}logP_{predicted}$).
    
- Using MLE w.r.t. $l(\theta)$.

    - $l(\theta) = \sum\limits_{i=1}^my^ilogh_\theta(x^i) + (1-y^i)log(1-h_\theta(x^i))$
    - Using (Batch) Gradient Ascent to chose $\arg\max\limits_{\theta}l(\theta)$.
        - $\theta_j = \theta_j + \alpha\underbrace{\sum\limits_{i=1}^m(y^i-h_\theta(x^i))x^i_j}_{\frac{\partial(\theta)}{\partial \theta_j}}$ 
        - This is actually **the same as** linear regression.
    - $l(\theta)$ w.r.t our $h_\theta(\cdot)$ will **always be concave**. (no local maximum)

## 3.5. Newton's Method

- Want to maximize $l(\theta)$ == want $l'(\theta) = 0$
- Newton's Method might be an alternative for gradient descent/ascent.
    - It requires much less iteration (fast), but more expensive at each iteration when $\theta$ is high dimensional.
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200811130027390.png" alt="image-20200811130027390" style="zoom:50%;" />
- When parameters are small, do use Newton's Method.

# Lecture 4 - Perceptron & Generalized Linear Model

## 4.1. Perceptron (感知机)

- The perceptron **HARDLY USED IN PRACTICE.**
    - It's kind of arbitrary and intuitive. 

- In binary classification: $h_\theta(x) = g(\theta^Tx)$
    - Linear regression uses $g(z) = \frac{1}{1+e^{-z}}$, perceptron uses $g(x) = \left\{ \begin{aligned} &1, &z \geq0\\ &0, &z < 0\end{aligned}\right.$
-  $\theta_j = \theta_j + \alpha\underbrace{\sum\limits_{i=1}^m(y^i-h_\theta(x^i))x^i_j}_{\frac{\partial(\theta)}{\partial \theta_j}}$, in perceptron, $(y^i-h_\theta(x^i))$ can be only $\{0, 1, -1\}$.
- <img src="./pic/QQMail_0.png" alt="perceptron" style="zoom:33%;" />
    - So basically, you want $\theta$ to be close to $x$ when $y = 1$ (*this way for all $y = 1$, corresponding $x$ will all be one side of the decision boundary*), and away from $x$ when $y = 0$.

## 4.2. Exponential Family

- Their PDF can be written as $p(y;\eta) = b(y)exp(\eta^TT(y) - a(\eta))$
    - where : $y$ is the **data**; (because in GLM, we actually want to model the output given input)
    - $\eta$ is **natural parameter**; (parameter of the distribution)
    - $T(y)$ is **sufficient statistic** of $y$;
    - $b(y)$ is **base measure**; (scalar)
    - $a(\eta)$ is **log-partition**. (scalar)
    - <font color="grey">sufficient statistics: 充分统计量 (对于未知参数的估计问题，保留了原始样本中关于未知参数θ的全部信息的统计量，就是充分统计量); 这里T(y)可以被理解为y</font>
- You can arbitrarily choose $T,b,a$ as long as PDF integrates to 1. 
- E.g. Bernoulli $p(y;\phi) = \phi^y(1-\phi)^{1-y}$
    - $p(y;\phi) = 1\cdot exp[log(\frac{\phi}{1-\phi})y + log(1-\phi)]$
        - $\eta=log(\frac{\phi}{1-\phi}) \rightarrow \phi = \frac{1}{1+e^{-\eta}}; a(\eta) = -log(1-\phi) = -\log(1-\frac{1}{1+e^{-\eta}})$.
- E.g. Gaussian/ Poisson/ Gamma/ Exponential/ Beta/ Dirichlet distribution.
    - see [here](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf) (might have proof)
    - In Gaussian $N(\mu, 1)$. $\eta = \mu; a(\eta) = \frac{\mu^2}{2}$

### Properties of exponential family

- MLE w.r.t. $\eta$ is concave. NLL (negative log likelihood) is convex.
- $\mathbb{E[y;\eta]} = \frac{\partial a(\eta)}{\partial \eta}$
- $Var[y;\eta] = = \frac{\partial^2 a(\eta)}{\partial^2 \eta}$

## 4.3. Generalized Linear Models (GLM)

- Assumptions / Design choices:
    - $(y|x;\theta) \sim$ Exponential Family $(\eta)$. 
    - $\eta = \theta^Tx; \theta\in\mathbb{R}^n, x\in\mathbb{R}^n$
- How to train / test:
    - Test time: output $\mathbb{E}[y|x;\theta]$, in other words. $h_\theta(x) = \mathbb{E}[y|x;\theta] = \mathbb{E}[y;\eta] = \mathbb{E}[y;\theta^Tx]$.
    - Train time: $\max\limits_\theta logp(y^i|\theta^Tx)$

- <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/glm.png" alt="GML" style="zoom:20%;" />
- **Learning update rule**: (the same as linear regression / logistic regression)
    - $\theta_j = \theta_j + \alpha\sum\limits_{i=1}^m(y^i-h_\theta(x^i))x^i_j$ 
- Terminologies:
    - $\eta$ is natural parameter
    - $ g(\eta) \triangleq \mathbb{E}[y;\eta]$ is called canonical response function (正则响应函数)
        - and $\mathbb{E[y;\eta]} = \frac{\partial a(\eta)}{\partial \eta}$ for exponential families.
    - $\eta = g^{-1}(\mathbb{E}[y;\eta])$ is called canonical link function. (正则关联函数)
- 3-parameterization spaces: 
    - model parameter: $\theta$;
    - natural parameter $\eta$;
    - canonical parameter: $\phi$ for Bernoulli; $(\mu, \sigma)$ for Gaussian; $\lambda$ for Poisson.
    - where in GLM, $\eta = \theta^Tx$ is our **design choice**, $g$ is the link between $\eta$ and canonical parameters.
        - $g(\eta) = \mathbb{E}[y;\eta] = ...$, and $\mathbb{E}[y;\eta] = f(\text{canonical})$.
        - More intuitively, $\eta$ and $\text{canonical}$ are just parameters of your distribution, so if you write the distribution pdf as the form of exponential family, you can directly get the relationship between $\eta$ and $\text{canonical}$s.
            - e.g. In Bernoulli, $\phi = \frac{1}{1+e^{-\eta}}$.

### What is GLM doing:

- Actually in linear regression, you **actually assume** the response $(y|x;\theta)$ is Gaussian, while this is quite limited.
    - ![glm2](/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/glm2.png)
    - see [here](https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab) (How to **choose distribution fit to your data**)
- For $(y|x;\theta)$, you **can choose** a distribution that you want $(y|x;\theta)$ to following (exponential family), then you can automatically get a GLM and start training.
- I.E. If you want to $(y|x;\theta) \sim $ Bernoulli $\phi$:
    - In Bernoulli $\phi = \frac{1}{1+e^{-\eta}}$.
    - then you get $h_\theta(x) = \mathbb{E}[y;\eta] = \mathbb{E}[y|x; \theta] = \phi$ (The expectation of Bernoulli is $\phi$).
    - then you get $h_\theta(x) = \phi = \frac{1}{1+e^{-\eta}} = \frac{1}{1+e^{-\theta^Tx}}$. (where $\eta = \theta^Tx$ is your **design choice**.)

## 4.4. Softmax Regression (Multi-class Classification)

- $k$ classes, $y$ is a one-hot vector $\in\mathbb{R}^k$, and $x\in\mathbb{R}^n$, so $\theta \in \mathbb{R}^{n\times k}$.
    - It's like doing linear regression to predict the probability of $x$ to be each class. (after Softmax)
        - $\begin{bmatrix}\theta^1x\\ \vdots\\\theta^kx\\ \end{bmatrix} = \begin{bmatrix}p(x\in\text{class}^1)\\\vdots\\ p(x\in\text{class}^k)\end{bmatrix}$
    - Then do linear regression with cross-entropy.

## 4.5. TODO 看泊松之类的是什么.

# Lecture 5 - GDA & Naive Bayes

- Previous methods are all **discriminative learning algorithms**, today we are going to talk about **generative learning algorithms**. (GDA)
    - A discriminative model learns $p(y|x)$; or $ h_\theta = \left\{\begin{aligned}0\\1\end{aligned} \right.$ directly.
    - A generative learning algorithm learns $p(x|y)$ and $\underbrace{p(y)}_{\text{class prior}}$.
        - Bayes rule: $p(y = 1| x) = \frac{p(x | y = 1)p(y = 1)}{p(x)} = \frac{p(x | y = 1)p(y = 1)}{\sum\limits_{y_0\in Y}p(x,y=y_0)} = \frac{p(x | y = 1)p(y = 1)}{\sum\limits_{y_0\in Y}p(x|y)p(y_0)} $

## 5.1. Gaussian Discriminant Analysis (GDA)

- Suppose $x \in \mathbb{R}^n$ (drop $x_0 = 1$ convention in lecture 2), assume $p(x|y)$ is **Gaussian**.
- Multi-variable $z \sim \mathbb{R}^n, z\sim N(\mu, \Sigma)$, $Cov(z) = E((z-\mu)(z-\mu)^T) = E(zz^T) - E(z)E^T(z)$, $p(z) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(z-\mu)^T\Sigma^{-1}(z-\mu))$.
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200816180503820.png" alt="image-20200816180503820" style="zoom:25%;" /> <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200816180751368.png" alt="image-20200816180751368" style="zoom:25%;" />

### 5.1.1. GDA model

- Assume model is Gaussian, i.e. $p(x|y)$ is Gaussian. (Let's use binary classification as an example.)
    - Assume $\Sigma1 == \Sigma_2 == \Sigma$
        - **By doing so, the decision boundary is linear.**
    - Parameters: $\mu_0, \mu_1, \Sigma, \phi$
    
- $p(x|y = 0) =\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))$, $p(x|y = 1) =\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))$,   

    $p(y) = \phi^y(1-\phi)^{1-y}$.

- How to fit the parameters? --> Maximize the joint likelihood.
    - $L(\phi, \mu_0, \mu_1, \Sigma) = \mathop\Pi\limits_{i=1}^m\underbrace{p(x^i,y^i;\phi, \mu_0, \mu_1, \Sigma)}_{\text{joint}}$.
        - While in discriminative models: $L(\theta) = \mathop\Pi\limits_{i=1}^m\underbrace{p(x^i|y^i;\theta)}_{\text{conditional}}$.
    
- Using MLE to $log(L(\phi, \mu_0, \mu_1, \Sigma))$:
    
    - $\phi=\frac{\sum\limits_{i=1}^m I(y^i=1)}{m}$, $\mu_t = \frac{\sum\limits_{i=1}^m I(y^i=t)x^i}{\sum\limits_{i=1}^m I(y^i=t)}$, $\Sigma = \frac{1}{m}\sum\limits_{i=1}^m(x^i-\mu_{y^i})(x^i-\mu_{y^i})^T$.
    
- Using GDA to predict:
    
    - $\max\limits_yp(y|x) = \arg\max\limits_{y}\frac{p(x|y)p(y)}{p(x)}$; while $p(x)$ is **constant** w.r.t. $y$. (normalizer)

GDA vs logistic regression:

- GDA: $p(y=1|x; \phi, \mu_0, \mu_1, \Sigma) = \frac{p(x|y=1;\mu_1, \Sigma)p(y=1;\phi)}{p(x;\phi,\mu_0,\mu_1,\phi)}$.
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200816183358711.png" alt="image-20200816183358711" style="zoom:33%;" />
    - Actually a  **sigmoid** function.

## 5.2. Generative vs Discriminative

- Generative vs Discriminative ([link](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3#:~:text=In%20General%2C%20A%20Discriminative%20model,P(Animal%20%7C%20Features).&text=A%20Generative%20Model%20%E2%80%8Clearns%20the,p(x%2Cy).))
    
    > In General, A Discriminative model ‌models the **decision boundary between the classes**. A Generative Model ‌explicitly models the **actual distribution of each class**. 
    >
    > <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200912153459601.png" alt="image-20200912153459601" style="zoom:50%;" />
    
- GDA's Assumption: (1)
    
    - $x|y=0\sim N(\mu_0,\Sigma)$
    - $x|y=1\sim N(\mu_1,\Sigma)$
    - $y\sim \text{Bernoulli}(\phi)$
    
- Logistic regression's Assumption: (2)
    
    - $p(y=1|x) = \frac{1}{1+e^{-\theta^Tx}}$.
    
- $(1)\rightarrow(2)$; $(2)\nrightarrow(1)$:
    - GDA is actually making a **much stronger assumptions**.
    - So if this stronger assumption *is **roughly correct***, then GDA will probably do better. (especially when the data set is small.)
    
- $\left.\begin{aligned}x|y = 0 &\sim \text{Possion}(\lambda_0)\\x|y = 1 &\sim \text{Possion}(\lambda_1)\\y &\sim \text{Bernoulli}(\phi)\end{aligned} \right\} \rightarrow p(y=1|x)\text{ is logistic}.$
    
    -   If $x|y=i$ comes from the same exponential distribution family (only varies in natural parameters), then $p(y=i|x)$ will be in the form of logistic.

## 5.3. Naive Bayes (part 1)

- *The email spam problem.*
- Feature vector $x$ (bag of words) $\{0,1\}^n$, $y\in \{0,1\}$:
    - If we want to model $x$ (has $2^n$ possible values) with a multinomial distribution, we will need $2^n-1$ parameters. (*-1* because probability adds up to 1.)
    - Note: $x|y$ might not following the Gaussian distribution.
- **Naive Bayes Assumption**:
    - $x_i$ is conditionally independent given $y$.
    - $p(x_1, \cdots, x_n | y) = p(x_1|y)p(x_2|y, x_1)\cdots p(x_n|y, x_1, x_2, \cdots, x_{n-1}) \triangleq p(x_1|y)p(x_2|y)\cdots p(x_n|y)$.
- $\begin{aligned}\phi_{j|y_0} &= p(x_j=1|y=y_0) \\ \phi_y &= p(y=1) \end{aligned}$.
- Using MLE to maximize joint likelihood $p(x,y;\phi_y,\phi_{j|y})$
    - $\phi_y=\frac{\sum\limits_{i=1}^m I(y^i=1)}{m}$, $\phi_{j|y=1} = \frac{\sum\limits_{i=1}^m I(x^i_j=1, y^i=1))}{\sum\limits_{i=1}^m I(y^i=1)}$.
    - Problem might happen when the denominator is zero.
    - $p(y=y_0 | x) = \frac{p(x|y=y_0)p(y=y_0)}{p(x)} \propto p(x|y=y_0)p(y=y_0) $

# Lecture 6 - Support Vector Machines

## 6.1. Naive Bayes (part 2)

### 6.1.1. Laplace Smoothing

### 6.1.2. Event Models for Text Classification

## 6.2. Comments on Apply ML

## 6.3. SVM Introduction





# Lecture 14 - Expectation-Maximization Algorithms

## 14.1 K-means
$$
\mathcal{D}: \{x^1, \cdots, x^N\}\\
\text{Initialize cluster centrals: randomly select k samples from your dataset.}\\
\text{Clustering each data according to: } c^i = \arg\min\limits_{j=1:k}||x^i-\mu_j||_2\\
\text{Calculate the mean for each cluster: } \mu_j = \frac{\sum\limits_{i=1:N}I(c^i=j)x^i}{\sum\limits_{i=1:N}I(c^i=j)}
$$
- The initialization should make sure that each cluster center is non-empty for the first clustering.
- The cost function $L(c,\mu) = \sum\limits_{i=1:N}||x^i-\mu_{c^i}||_2$, k-means will drag this cost function down.
    - K-meas will stick at local minimal, so you should run that multiple times with random initialization.

## 14.2 Mixture Gaussian

- Anomaly detection: model $p(x)$, then if $p(x) \leq \delta$, chances are that it's anomaly.
- <img src="./pic/image-20200730165438776.png" alt="image-20200730165438776" style="zoom:50%;" />
    - How to model an 'L' distribution? --> GMM.

- 1-D example:
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730165716539.png" alt="image-20200730165716539" style="zoom: 50%;" />

### Mixture of Gaussian Model

- Suppose there's a hidden/latent random variable $z$, and $x^i,z^i$ are jointly distributed ($m$ samples)
    - $p(x^i,z^i) = p(x^i|z^i)p(z^i)$

- where $z^i\sim \text{Multinomial}(\phi)$ (多项式分布), $z\sim\{1,\cdots,k\}$ , and $(x^i|z^i=j)\sim N(\mu_j,\Sigma_j)$
    - 多项式分布就是二项式分布(伯努利分布)从n=2到n=k
    - $\phi = \{\cdots, \phi_j\}$, and $\sum_j{\phi_j}= 1$
- **If we know $z^i$s,** we can use MLE: 
    - $L(\phi,\mu,\Sigma) = \sum\limits_{i=1}^mlogp(x^i,z^i|\phi,\mu,\Sigma)$
    - then we can get $\phi_j = \frac{1}{m}\sum\limits_{i=1}^mI(z^i=j)$, $\mu_j=\frac{\sum\limits_{i=1}^mI(z^i=j)x^i}{\sum\limits_{i=1}^mI(z^i=j)}$, $\Sigma_j = \cdots$

### M for GMM (expectation maximization)

- E-step: (Guess value of $z^i$s)
    - Set $\begin{aligned}\omega_j^i &= p(z^i=j|x^i,\phi,\mu,\Sigma)\\ &= \frac{p(x^i|z^i=j)p(z^i=j)}{\sum\limits_{l=1}^kp(x^i|z^i=l)p(z^i=l)}\end{aligned}$, $\omega^i_j$ means the probability of $x_i$ is coming from Gaussian$_j$ 
    - where $p(x^i|z^i=j)$ is Gaussian, and $p(z^i=j) = \phi_j$
- M-step: (Updates the parameters of our model based on our guesses)
    - $\begin{aligned}\phi_{j} &= \frac{1}{m}\sum\limits_{i=1}^{m}\omega_j^i\\ \mu_j &= \frac{\sum\limits_{i=1}^m\omega^i_jx_i}{\sum\limits_{i=1}^m\omega^i_j}\\ \Sigma_j &= \cdots\end{aligned}$
- So at **E-step** you estimate every $\omega^i_j$ with some given $\phi,\mu,\Sigma$, and at **M-step** you update  $\phi,\mu,\Sigma$ with rules from MLE.
    - Then you do these two steps in a loop: bootstrap.
    - $\phi,\mu,\Sigma$ might be random initialized at first.

- Finally $p(x_i) = \sum\limits_{i=1}^mp(x_i,z_i)$

- k-means is a hard clustering methods (each point will be assigned to one cluster)
- EM is a soft clustering method (each point will be assigned to one cluster with some possibility)

## 14.4 EM Algorithm

### Jensen's inequality

- Let $f$ be a convex function ($f''(x) \geq 0$), let $X$ to be a random variable, then $f[E(X)] \leq E[f(x)]$
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730174717027.png" alt="image-20200730174717027" style="zoom:50%;" />
- Further, if $f''(x) > 0$ , f is strictly convex. 
    - then $f[E(X)] = E[f(X)] \iff X=\text{constant}\ (X = E[X]\text{ with probability 1})$
- And the whole thing works with concave functions, e.g. $\text{log}$ function.

### Density Estimation Problem (estimate P(x))

- Have model for $P(x,z;\theta)$, only observed $x$, **hidden variable $z$,** $\mathcal{D} = \{x^1, \cdots, x^m\}$

- $L(\theta) = \sum\limits_{i=1}^mlog\ p(x^i; \theta) = \sum\limits_{i=1}^mlog\sum\limits_{z^i}p(x^i,z^i; \theta)$

- **Want**: $\arg\max_{\theta}L(\theta)$

####  A glimpse of how we to that:

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730180343216.png" alt="image-20200730180343216" style="zoom:50%;" />

- At each E-step $i$ we draw a curve$^i$: 
    - The curve is the lower bound for the ground truth.
    - The curve equals to the ground truth at $\theta^{i-1}$
- At each M-step $i$ we move the $\theta^{i-1}$ to the optimal position $\theta^{i}$ of current curve$^i$
- The EM algorithm **will** converge to the local optimal.

#### A formal way of how we do that:

- The E-step
- E1: To get the lower bound:
$$
\begin{aligned}
&\arg\max_{\theta}\sum_{i}log\ p(x^i;\theta)\\
&= \sum\limits_ilog\sum\limits_{z^i}p(x^i,z^i;\theta)\\
&= \sum\limits_ilog\sum\limits_{{z^i}}Q_i(z^i)\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\ & \text{ ,where } Q_i(z^i) \text{ is an (arbitrary) probability distribution, } \sum\limits_{z^i}Q_i(z^i)=1\\
&= \sum\limits_ilog\mathop{\mathbb{E}}\limits_{z^i\sim Q_i}\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\
& \text{Using Jensen's inequality, the log function is concave}\\
& \geq \sum\limits_i\mathop{\mathbb{E}}\limits_{z^i\sim Q_i}log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}\\
& = \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\
& \text{Note: this function is just about }\theta \\
\end{aligned}
$$

- E2: To make the two curves **equal** at current iteration parameter $\theta$:
    - We need $\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} = \text{constant}$
    - We can set $Q_i(z^i) \propto P(x^i,z^i;\theta)$, and we need $\sum\limits_{z^i}Q_i(z^i)=1$:
        - One solution: $\begin{aligned}Q_i(z^i) &= \frac{P(x^i,z^i;\theta)}{\sum\limits_{z^i}P(x^i,z^i;\theta)}\\ &= P(z^i| x^i;\theta) \end{aligned}$

- The M-step:
    - $\theta = \arg\max\limits_{\theta} \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}$

#### A brief summary:

- E-step:
    - Set $Q_i(z^i) = P(z^i| x^i;\theta)$
- M-step:
    - $\theta = \arg\max\limits_{\theta} \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}$

### Converge proof

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730185849150.png" alt="image-20200730185849150" style="zoom:67%;" />

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730185946341.png" alt="image-20200730185946341" style="zoom:67%;" />

# Lecture 15 - EM Algorithm & Factor Analysis

