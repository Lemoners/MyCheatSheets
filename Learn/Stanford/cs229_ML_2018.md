Video: https://www.youtube.com/watch?v=rVfZHWTwXSA&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU

Notes: http://cs229.stanford.edu/syllabus-autumn2018.html



# Lecture 3 - Locally Weighted & Logistic Regression

## 3.1. Linear Regression (Recap)

- $$
    \begin{aligned}
    (x^i,y^i)->&\text{i}th \text{ example}\\
    x^i \in \mathbb{R}^{n+1}&, y^i\in\mathbb{R}\\
    m = \#example&, y =\#features\\
    h_{\theta}(x)=\sum\limits_{j=0}^n \theta_jx_j &= \theta^Tx,  \text{where }x_0 \triangleq 1, \text{for that }x_0\theta_0 \triangleq b \\
    J(\theta) = \frac{1}{2}\sum\limits_{i=1}^m&(h_\theta(x^i)-y^i)^2\\
    \end{aligned}
    $$
    
- The professor use a fake feature $x_0 = 1$, which we usually refer to as the bias $b$.

## 3.2. Locally Weighted (Linear) Regression

- Parametric Learning algorithm:

    - Fix **fixed** set of parameters $\theta_i$ for data. e.g. Linear regression.

- Non-parametric Learning algorithm:

    - Amount of **data**/**parameters** you need to **keep** grows (linearly) with the size of data.

- Locally weighted regression:

    - (Fit $\theta$ with **data close to your prediction point**)

        - Maybe think about the case where you want to fit a non-linear with linear regression, it's more reasonable to only focus on the neighbourhood.

    - $$
        \text{Fit }\theta\text{ to minimize}\\
        \sum\limits_{i=1}^m\omega^i(y^i-\theta^Tx^i)^2\\
        \text{where } \omega^i \text{ is a "weight" function}, \text{ a common choice is :}\\
        \omega^i = exp(-\frac{(x^i-x)^2}{2\tau^2})\\
        \text{where we called $\tau$: "bandwidth", when $\tau$ is small, you put more attention on the local area.}\\
        $$

- It's useful when you have tons of examples.

## 3.3. Probabilistic  Interpretation

- Why least square errors $||y^i-\theta^Tx^i||$?

- $$
    \text{Assume $y^i=\theta^Tx^i+\epsilon$, where $\epsilon$ stands for noise}\\
    $$

- 

## 3.4. Logistic Regression

## 3.5. Newton's Method

# Lecture 14 - Expectation-Maximization Algorithms

## 14.1 K-means
$$
\mathcal{D}: \{x^1, \cdots, x^N\}\\
\text{Initialize cluster centrals: randomly select k samples from your dataset.}\\
\text{Clustering each data according to: } c^i = \arg\min\limits_{j=1:k}||x^i-\mu_j||_2\\
\text{Calculate the mean for each cluster: } \mu_j = \frac{\sum\limits_{i=1:N}I(c^i=j)x^i}{\sum\limits_{i=1:N}I(c^i=j)}
$$
- The initialization should make sure that each cluster center is non-empty for the first clustering.
- The cost function $L(c,\mu) = \sum\limits_{i=1:N}||x^i-\mu_{c^i}||_2$, k-means will drag this cost function down.
    - K-meas will stick at local minimal, so you should run that multiple times with random initialization.

## 14.2 Mixture Gaussian

- Anomaly detection: model $p(x)$, then if $p(x) \leq \delta$, chances are that it's anomaly.
- <img src="./pic/image-20200730165438776.png" alt="image-20200730165438776" style="zoom:50%;" />
    - How to model an 'L' distribution? --> GMM.

- 1-D example:
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730165716539.png" alt="image-20200730165716539" style="zoom: 50%;" />

### Mixture of Gaussian Model

- Suppose there's a hidden/latent random variable $z$, and $x^i,z^i$ are jointly distributed ($m$ samples)
    - $p(x^i,z^i) = p(x^i|z^i)p(z^i)$

- where $z^i\sim \text{Multinomial}(\phi)$ (多项式分布), $z\sim\{1,\cdots,k\}$ , and $(x^i|z^i=j)\sim N(\mu_j,\Sigma_j)$
    - 多项式分布就是二项式分布(伯努利分布)从n=2到n=k
    - $\phi = \{\cdots, \phi_j\}$, and $\sum_j{\phi_j}= 1$
- **If we know $z^i$s,** we can use MLE: 
    - $L(\phi,\mu,\Sigma) = \sum\limits_{i=1}^mlogp(x^i,z^i|\phi,\mu,\Sigma)$
    - then we can get $\phi_j = \frac{1}{m}\sum\limits_{i=1}^mI(z^i=j)$, $\mu_j=\frac{\sum\limits_{i=1}^mI(z^i=j)x^i}{\sum\limits_{i=1}^mI(z^i=j)}$, $\Sigma_j = \cdots$

### M for GMM (expectation maximization)

- E-step: (Guess value of $z^i$s)
    - Set $\begin{aligned}\omega_j^i &= p(z^i=j|x^i,\phi,\mu,\Sigma)\\ &= \frac{p(x^i|z^i=j)p(z^i=j)}{\sum\limits_{l=1}^kp(x^i|z^i=l)p(z^i=l)}\end{aligned}$, $\omega^i_j$ means the probability of $x_i$ is coming from Gaussian$_j$ 
    - where $p(x^i|z^i=j)$ is Gaussian, and $p(z^i=j) = \phi_j$
- M-step: (Updates the parameters of our model based on our guesses)
    - $\begin{aligned}\phi_{j} &= \frac{1}{m}\sum\limits_{i=1}^{m}\omega_j^i\\ \mu_j &= \frac{\sum\limits_{i=1}^m\omega^i_jx_i}{\sum\limits_{i=1}^m\omega^i_j}\\ \Sigma_j &= \cdots\end{aligned}$
- So at **E-step** you estimate every $\omega^i_j$ with some given $\phi,\mu,\Sigma$, and at **M-step** you update  $\phi,\mu,\Sigma$ with rules from MLE.
    - Then you do these two steps in a loop: bootstrap.
    - $\phi,\mu,\Sigma$ might be random initialized at first.

- Finally $p(x_i) = \sum\limits_{i=1}^mp(x_i,z_i)$

- k-means is a hard clustering methods (each point will be assigned to one cluster)
- EM is a soft clustering method (each point will be assigned to one cluster with some possibility)

## 14.4 EM Algorithm

### Jensen's inequality

- Let $f$ be a convex function ($f''(x) \geq 0$), let $X$ to be a random variable, then $f[E(X)] \leq E[f(x)]$
    - <img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730174717027.png" alt="image-20200730174717027" style="zoom:50%;" />
- Further, if $f''(x) > 0$ , f is strictly convex. 
    - then $f[E(X)] = E[f(X)] \iff X=\text{constant}\ (X = E[X]\text{ with probability 1})$
- And the whole thing works with concave functions, e.g. $\text{log}$ function.

### Density Estimation Problem (estimate P(x))

- Have model for $P(x,z;\theta)$, only observed $x$, **hidden variable $z$,** $\mathcal{D} = \{x^1, \cdots, x^m\}$

- $L(\theta) = \sum\limits_{i=1}^mlog\ p(x^i; \theta) = \sum\limits_{i=1}^mlog\sum\limits_{z^i}p(x^i,z^i; \theta)$

- **Want**: $\arg\max_{\theta}L(\theta)$

####  A glimpse of how we to that:

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730180343216.png" alt="image-20200730180343216" style="zoom:50%;" />

- At each E-step $i$ we draw a curve$^i$: 
    - The curve is the lower bound for the ground truth.
    - The curve equals to the ground truth at $\theta^{i-1}$
- At each M-step $i$ we move the $\theta^{i-1}$ to the optimal position $\theta^{i}$ of current curve$^i$
- The EM algorithm **will** converge to the local optimal.

#### A formal way of how we do that:

- The E-step
- E1: To get the lower bound:
$$
\begin{aligned}
&\arg\max_{\theta}\sum_{i}log\ p(x^i;\theta)\\
&= \sum\limits_ilog\sum\limits_{z^i}p(x^i,z^i;\theta)\\
&= \sum\limits_ilog\sum\limits_{{z^i}}Q_i(z^i)\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\ & \text{ ,where } Q_i(z^i) \text{ is an (arbitrary) probability distribution, } \sum\limits_{z^i}Q_i(z^i)=1\\
&= \sum\limits_ilog\mathop{\mathbb{E}}\limits_{z^i\sim Q_i}\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\
& \text{Using Jensen's inequality, the log function is concave}\\
& \geq \sum\limits_i\mathop{\mathbb{E}}\limits_{z^i\sim Q_i}log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}\\
& = \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} \\
& \text{Note: this function is just about }\theta \\
\end{aligned}
$$

- E2: To make the two curves **equal** at current iteration parameter $\theta$:
    - We need $\frac{P(x^i,z^i;\theta)}{Q_i(z^i)} = \text{constant}$
    - We can set $Q_i(z^i) \propto P(x^i,z^i;\theta)$, and we need $\sum\limits_{z^i}Q_i(z^i)=1$:
        - One solution: $\begin{aligned}Q_i(z^i) &= \frac{P(x^i,z^i;\theta)}{\sum\limits_{z^i}P(x^i,z^i;\theta)}\\ &= P(z^i| x^i;\theta) \end{aligned}$

- The M-step:
    - $\theta = \arg\max\limits_{\theta} \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}$

#### A brief summary:

- E-step:
    - Set $Q_i(z^i) = P(z^i| x^i;\theta)$
- M-step:
    - $\theta = \arg\max\limits_{\theta} \sum\limits_i\sum\limits\limits_{z^i\sim Q_i}Q_i(z^i)log\frac{P(x^i,z^i;\theta)}{Q_i(z^i)}$

### Converge proof

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730185849150.png" alt="image-20200730185849150" style="zoom:67%;" />

<img src="/home/lemon/Workspace/myCheatSheet/Learn/Stanford/pic/image-20200730185946341.png" alt="image-20200730185946341" style="zoom:67%;" />

# Lecture 15 - EM Algorithm & Factor Analysis

